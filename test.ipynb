{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95fb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/23 20:16:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETED - Summary Statistics\n",
      "======================================================================\n",
      "Execution time: 8.29 seconds\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  regexp_extract, col, count, rand\n",
    "# import pandas as pd\n",
    "import time\n",
    "\n",
    "# Setup \n",
    "INPUT_DIR = \"data/sample\" \n",
    "OUTPUT_DIR = \"data/output\"\n",
    "\n",
    "\n",
    "# Initialize Spark session (local)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Problem1_Local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read all text files from the input directory (recursively) into a Spark DataFrame\n",
    "logs_df = spark.read.option(\"recursiveFileLookup\", \"true\").text(INPUT_DIR)\n",
    "\n",
    "# Define a regex pattern to capture log levels (INFO, WARN, ERROR, DEBUG)\n",
    "pattern = r\"(INFO|WARN|ERROR|DEBUG)\"\n",
    "\n",
    "# Extract the log level from each line and store it in a new column\n",
    "logs_parsed = logs_df.withColumn(\n",
    "    'log_level', regexp_extract('value', pattern, 1)\n",
    "    )\n",
    "\n",
    "# Keep only rows that contain a valid log level\n",
    "valid_df = logs_parsed.filter(col(\"log_level\") != \"\")\n",
    "\n",
    "# Count how many times each log level appears\n",
    "counts_df = valid_df.groupBy(\"log_level\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Save the counts for each log level as a CSV file\n",
    "counts_df.toPandas().to_csv(f'{OUTPUT_DIR}/problem1_counts.csv', index=False)\n",
    "\n",
    "# Randomly select 10 log entries to create a sample of the data\n",
    "sample_df = valid_df.orderBy(rand()).limit(10)\n",
    "\n",
    "# Rename the text column to 'log_entry' for clarity in the sample file\n",
    "sample_df = sample_df.withColumnRenamed('value', 'log_entry')\n",
    "\n",
    "# Save the random sample as a CSV file\n",
    "sample_df.toPandas().to_csv(f'{OUTPUT_DIR}/problem1_sample.csv', index=False)\n",
    "\n",
    "# Calculate total numbers of lines and valid log entries, and number of unique levels\n",
    "total_lines = logs_df.count()\n",
    "total_valid = valid_df.count()\n",
    "unique_levels = counts_df.count()\n",
    "\n",
    "# Create a summary report including totals and log level distribution\n",
    "summary_text = (\n",
    "    f\"Total log lines processed: {total_lines}\\n\"\n",
    "    f\"Total lines with log levels: {total_valid}\\n\"\n",
    "    f\"Unique log levels found: {unique_levels}\\n\"\n",
    ")\n",
    "\n",
    "# Write the summary text and log level distribution with percentages to a file\n",
    "with open(f'{OUTPUT_DIR}/problem1_summary.txt', \"w\") as f:\n",
    "    f.write(summary_text)\n",
    "    f.write(\"Log level distribution:\\n\")\n",
    "    for _, row in counts_df.toPandas().iterrows():\n",
    "        level = row[\"log_level\"]\n",
    "        cnt = int(row[\"count\"])\n",
    "        pct = (cnt / total_valid) * 100 if total_valid > 0 else 0.0\n",
    "        f.write(f\"  {level:<5}: {cnt:10,} ({pct:6.2f}%)\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Time Summary\n",
    "execution_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETED - Summary Statistics\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-spark-cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
